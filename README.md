# Paper
## Title
OmniGeo: Towards a Multimodal Large Language Models for Geospatial Artifcial Intelligence
## Abstract
The rapid advancement of multimodal large language models (LLMs) has opened new frontiers in artifcial intelligence, enabling the integration of diverse largescale data types such as text, images, and spatial information. However, the vanilla multi-task learning causes conﬂicts, due to the signifcant differences across geospatial tasks. To address this gap, in this paper, we frst explore the potential of multimodal LLMs (MLLM) for geospatial artifcial intelligence (GeoAI), a feld that leverages spatial data to address challenges in domains including Geospatial Semantics, Health Geography, Urban Geography, Urban Perception, and Remote Sensing. To mitigate conﬂicts, we fuse images with captions as anchor into multi-task learning and propose a MLLM (OmniGeo) tailored to geospatial applications, capable of processing and analyzing heterogeneous data sources, including satellite imagery, geospatial metadata, and textual descriptions. By combining the strengths of natural language understanding and spatial reasoning, our model enhances the ability of instruction following and the accuracy of GeoAI systems. Results demonstrate that our model outperforms task-specifc models and existing LLMs on diverse geospatial tasks, effectively addressing the multimodality nature while achieving competitive results on the zero-shot geospatial tasks.

<img width="1423" height="703" alt="image" src="https://github.com/user-attachments/assets/e7ab8029-8adf-4d40-932f-7acbdf5e14f5" />



# Data
## Test Data
通过网盘分享的文件：OmniGeo
链接: https://pan.baidu.com/s/1ZIo0iPAfGEwct_CXv4yUHQ?pwd=uttk 提取码: uttk

# Code
Our code comes from the repository : llama-factory https://github.com/hiyouga/LLaMA-Factory
